stage_1:
  # select from [rae, da3, vggt]
  model: da3
  rae:
    target: stage1.RAE
    params:
      encoder_cls: 'Dinov2withNorm'
      encoder_config_path: 'facebook/dinov2-with-registers-base'
      encoder_input_size: 448
      encoder_params: {'dinov2_path': 'facebook/dinov2-with-registers-base', 'normalize': True}
      decoder_config_path: 'RAE/configs/decoder/ViTXL'
      pretrained_decoder_path: 'RAE/models/decoders/dinov2/wReg_base/ViTXL_n08_i512/model.pt'
      noise_tau: 0.
      reshape_to_2d: True
      normalization_stat_path: 'RAE/models/stats/dinov2/wReg_base/imagenet1k_512/stat.pt'
  da3:
    # ckpt: depth-anything/DA3NESTED-GIANT-LARGE
    ckpt: depth-anything/DA3-GIANT-1.1
    target:
    params:
  vggt:
    ckpt: facebook/VGGT-1B
    target:
    params:

stage_2:

  model: DiTwDDTHeadMVRM
  target: stage2.models.DDT_MVRM.DiTwDDTHeadMVRM
  params:
    input_size: [27, 36]  # feat_H, feat_W
    patch_size: 1
    # in_channels: 1536
    # hidden_size: [1536, 2048]
    in_channels: 3072
    hidden_size: [1536, 3072]
    depth: [28, 2]
    num_heads: [16, 16]
    mlp_ratio: 4.0
    class_dropout_prob: 0.1
    num_classes: 1000
    use_qknorm: false
    use_swiglu: true
    use_rope: true
    use_rmsnorm: true
    wo_shift: false
    use_pos_embed: true
  # ckpt: RAE/models/DiTs/Dinov2/wReg_base/ImageNet512/DiTDH-XL_ep400/stage2_model.pt


transport:
  params:
    path_type: 'Linear'
    prediction: 'velocity'
    loss_weight: null
    time_dist_type: 'logit-normal_0_1'

sampler:
  mode: ODE
  params:
    sampling_method: 'euler'
    num_steps: 50
    atol: 1.0e-6
    rtol: 1.0e-3
    reverse: false

guidance:
  method: 'cfg'
  scale: 1.0
  t_min: 0.0
  t_max: 1.0

misc:
  latent_size: [1, 3072, 27, 36] # [v, c, h, w]
  num_classes: 
  time_dist_shift_dim: 2985984 # 3072*27*36
  time_dist_shift_base: 8192

training:
  global_seed: 42
  precision: fp32
  epochs: 50
  global_batch_size: 1    # this is the final total batch size
  grad_accum_steps: 1     # by controling grad_accum_step, the batch_size is adjusted automatically
  ema_decay: 0.9995
  num_workers: 4
  shuffle: False
  log_interval: 100
  checkpoint_interval: 1
  sample_every: 1000
  clip_grad: 1.0
  optimizer:
    lr: 2.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.0
  scheduler:
    type: linear
    warmup_epochs: 25
    decay_end_epoch: 50
    base_lr: 2e-4
    final_lr: 2e-5
    warmup_from_zero: false

# eval:
#   eval_interval: 1 # Eval interval by optimization step
#   eval_model: true # By default only evaluates EMA model. Set to true to eval non-EMA model as well.
#   data_path: /media/data1/ImageNet2012/pho_val # path to ImageNet val images
#   reference_npz_path: /media/data1/ImageNet2012/pho_val.npz # packed npz of ImageNet val images for FID calculation
#   metrics: ['psnr', 'ssim', 'rfid'] # metrics to calculate


log:
  result_root_dir: result_train/stage2
  tracker:
    name: wandb 
    wandb:
      key: e32eed0c2509bf898b850b0065ab62345005fb73
      entity: jinlovespho
      project: eccv26_mv_restoration
      # msg: "hqview_recon"
      # msg: "lqview-kernel50"
      # msg: "lqview-kernel30"
      # msg: "lqview-kernel10"      
      # msg: "lqview-kernel100"      
      msg: "TMPTMPTMPTMP_LQ50"
data:
  train:
    image_size: 512 
    num_input_view: 1
    # list: [imagenet]
    list: [hypersim]
    # list: [tartanair]
    # list: [hypersim, tartanair,]
    
    imagenet:
      hq_root_path: /mnt/dataset1/ImageNet2012/train
      lq_root_path: 

    hypersim:
      hq_root_path: /mnt/dataset1/MV_Restoration/hypersim/data
      hq_latent_root_path: 
      # lq_root_path: /mnt/dataset1/MV_Restoration/hypersim/deg_blur/kernel10_intensity01
      # lq_root_path: /mnt/dataset1/MV_Restoration/hypersim/deg_blur/kernel30_intensity01
      lq_root_path: /mnt/dataset1/MV_Restoration/hypersim/deg_blur/kernel50_intensity01
      # lq_root_path: /mnt/dataset1/MV_Restoration/hypersim/deg_blur/kernel100_intensity01
      lq_latent_root_path: 
      depth_path: /mnt/dataset1/MV_Restoration/hypersim/data
      view_sel_strategy:
    
    tartanair:
      hq_root_path: /mnt/dataset1/MV_Restoration/tartanair/data
      hq_latent_root_path: 
      lq_root_path:
      lq_latent_root_path: 
      view_sel_strategy:
  val:
    image_size: 512 
    list: [eth3d]
    eth3d:
      hq_root_path:
      lq_root_path:
  

mvrm:
  # choose from [addition, concat]
  lq_latent_cond: addition
  train:
    # outputs = [feat19, feat27, feat33, feat39]
    extract_feat_layers: [19]
    restore_feat_layers: []
  val:
    extract_feat_layers: []
    restore_feat_layers: [19]
    
