stage_1:
  model: da3
  rae:
    target: 
    params:
      encoder_cls: 
      encoder_config_path: 
      encoder_input_size: 
      encoder_params: 
      decoder_config_path: 
      pretrained_decoder_path: 
      noise_tau: 
      reshape_to_2d: 
      normalization_stat_path: 
  da3:
    ckpt: depth-anything/DA3-GIANT-1.1
  vggt:
    ckpt:


stage_2:
  model: mvrm
  target: stage2.models.DDT_MVRM_multiview.DiTwDDTHeadMVRM_Multiview
  params:
    input_size: 
    patch_size: 1
    in_channels: 1536
    hidden_size: [1536, 3072]
    depth: [28, 2]
    num_heads: [16, 16]
    mlp_ratio: 4.0
    class_dropout_prob:
    num_classes: 
    use_qknorm: true 
    use_swiglu: true
    use_rope: true
    use_rmsnorm: true
    wo_shift: false
    use_pos_embed: true
  ckpt: 


transport:
  params:
    path_type: 'Linear'
    prediction: 'velocity'
    loss_weight: null
    time_dist_type: 'logit-normal_0_1'


sampler:
  mode: ODE
  params:
    sampling_method: 'euler'
    num_steps: 50
    atol: 1.0e-6
    rtol: 1.0e-3
    reverse: false


guidance:
  method: 'cfg'
  scale: 1.0
  t_min: 0.0
  t_max: 1.0


misc:
  latent_size: [1, 973, 1536] 
  num_classes: 
  time_dist_shift_dim: 1492992  
  time_dist_shift_base: 4096


training:
  global_seed: 42
  precision: fp32


  # SET TRAINING EPOCHS
  epochs: 10
  # SET GLOBAL BATCH SIZE
  global_batch_size: 16
  # SET NUMBER OF WORKERS TO GPU_NUM*4
  num_workers: 16
  # SET MODEL WEIGHT SAVING CKPT INTERVAL
  ckpt_step_interval: 1000   # 1k


  log_interval: 1
  shuffle: True
  grad_accum_steps: 1
  ema_decay: 0.9995
  vis:
    train_depth_every: -1
    val_depth_every: -1
  clip_grad: 1.0
  optimizer:
    lr: 2.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.0
  scheduler:
    type: linear
    warmup_epochs: 5
    decay_end_epoch: 10
    base_lr: 2e-4
    final_lr: 2e-5
    warmup_from_zero: false
  


# WANDB로깅 가능하시다면, 제껄로 로깅 해주시면 되는데,
# 보안상 어렵다면, 여기 log 블럭은 다 주석하시고 아래 log부분 주석 풀고 실행하시면 됩니다
log:
  # SET SAVING PATH
  result_root_dir: result_train/multi_view/JIHYE/lq_kernel50/260209
  tracker:
    name: wandb
    wandb:
      key: e32eed0c2509bf898b850b0065ab62345005fb73
      entity: jinlovespho
      project: eccv26_mv_restoration
      msg: "jihye_lq-kernel50"


# log:
#   # SET SAVING PATH
#   result_root_dir: result_train/multi_view/JIHYE/lq_kernel50/260209
#   tracker:
#     name: 
#     wandb:
#       key: 
#       entity: 
#       project: 
#       msg: "jihye_lq-kernel50"




data:
  train:
    image_size: 512 
    max_num_input_view: 8
    list: [hypersim, tartanair,]
    hypersim:
      ann_path: metadata_images_split_scene_v1.csv
      # SET HYPERSIM HQ ROOT PATH
      hq_root_path: /PATH/TO/HYPERSIM
      lq_kernel_size: 50
      view_selection:
        strategy: near_camera
        expand_ratio: 2
    tartanair:
      ann_path: 
      # SET TARTANAIR HQ ROOT PATH
      hq_root_path: /PATH/TO/TARTANAIR
      lq_kernel_size: 50
      view_selection:
        strategy: near_random 
        expand_ratio: 2


  val:
    image_size:  
    list: []
    eth3d:
      num_eval_img: 
      eval_scene: []
      ann_path:
      hq_root_path: 
      lq_root_path: 
      view_selection:
        strategy: 
        expand_ratio: 


mvrm:
  lq_latent_cond: addition   
  train:
    extract_feat_layers: [17]
    concat_feat: False
  val:
    restore_feat_layers: [17]
    concat_feat: False 
    