# DepthAnything3 Benchmark Evaluation Configuration
# 
# This config can be loaded and overridden via command line.
# Example: python -m depth_anything_3.bench.evaluator --model /path/to/model --work_dir /path/to/workspace
#
# See depth_anything_3.cfg for config utility functions.


# ==============================================================================
# MVRM Configuration
# ==============================================================================
MVRM_EVAL:
  # [wo_mvrm, w_mvrm, mvrm_up]
  eval_method: w_mvrm


# ==============================================================================
# Denoiser Configuration
# ==============================================================================
denoiser:
  model: DiTwDDTHeadMVRM_Multiview
  target: RAE.src.stage2.models.DDT_MVRM_multiview.DiTwDDTHeadMVRM_Multiview
  params:
    # input_size: [27, 36]  # feat_H, feat_W
    input_size: 
    patch_size: 1
    in_channels: 1536
    hidden_size: [1536, 3072]
    depth: [28, 2]
    num_heads: [16, 16]
    mlp_ratio: 4.0
    class_dropout_prob:
    num_classes: 
    use_qknorm: true 
    use_swiglu: true
    use_rope: true
    use_rmsnorm: true
    wo_shift: false
    use_pos_embed: true
  ckpt: NVIDIA_SERVER/TRAIN__fp32__hypersim-tartanair__near_camera-near_random__da3-mvrm__bs8-maxview8-accum1__lr-2e-04__msg-lqkernel-50__global-17__ddp/checkpoints/ep-0025000.pt

mvrm:
  lq_latent_cond: addition    # [addition, concat]
  train:
    # DPT Head input feats = [19, 27, 33, 39]
    extract_feat_layers: [17]
    # extract_feat_layers: [10]
    # extract_feat_layers: [5]
    concat_feat: False
    break_and_return_feat: true
  val:
    restore_feat_layers: [17]
    # restore_feat_layers: [10]
    # restore_feat_layers: [5]
    concat_feat: False 
    skip_and_input_feat: true

transport:
  params:
    path_type: 'Linear'
    prediction: 'velocity'
    loss_weight: null
    time_dist_type: 'logit-normal_0_1'
sampler:
  mode: ODE
  params:
    sampling_method: 'euler'
    num_steps: 50
    atol: 1.0e-6
    rtol: 1.0e-3
    reverse: false
misc:
  latent_size: 
  num_classes: 
  time_dist_shift_dim: 1492992  # 1536*27*36
  time_dist_shift_base: 4096

# ==============================================================================
# Model Configuration
# ==============================================================================
model:
  # Path to model checkpoint or HuggingFace model ID
  path: depth-anything/DA3-GIANT-1.1

# ==============================================================================
# Workspace Configuration
# ==============================================================================
workspace:
  # Working directory for outputs (model results, metrics, etc.)

  # MVRM X (clean)
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/clean/wo_mvrm/eval_full_view100
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/clean/wo_mvrm/eval_full_view50
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/clean/wo_mvrm/eval_full_view10

  # MVRM X (cam_blur_100)
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/wo_mvrm/cam_blur_100/eval_full_view100


  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/wo_mvrm/cam_blur_100/eval_full_view100

  
  # MVRM O

  # cam_blur_100


  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/w_mvrm/nvidia_giant_20k/cam_blur_100/eval_full_3
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/w_mvrm/nvidia_giant_20k/cam_blur_100/eval_full_100


  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/w_mvrm/nvidia_giant_20k/cam_blur_300/eval_full_100





  # MVRM UPPER BOUND
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/mvrm_upper/g17/cam_blur_100/eval_full_100
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/mvrm_upper/f10/cam_blur_100/eval_full_100
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/mvrm_upper/f5/cam_blur_100/eval_full_100
  # work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val/DA3/mvrm_upper/f0/cam_blur_100/eval_full_100


  work_dir: /mnt/dataset1/MV_Restoration/ECCV26_RESULTS/pho_result_val_TMP




# ==============================================================================
# Evaluation Configuration
# ==============================================================================
eval:
  # Datasets to evaluate
  # Options: dtu, dtu64, eth3d, 7scenes (sevenscenes), scannetpp, hiroom
  datasets:
    - eth3d
    - 7scenes
    - scannetpp
    - hiroom
    - dtu
    - dtu64

  # Evaluation modes
  # Options: pose, recon_unposed, recon_posed, view_syn
  modes:
    - pose
    - recon_unposed
    - recon_posed

  # Reference view selection strategy for inference
  # Options: first, saddle_balanced, auto, mid
  ref_view_strategy: "first"

  # Specific scenes to evaluate (null = all scenes)
  # Example: [courtyard, relief] for eth3d
  scenes: null

  # Maximum number of frames per scene (for sampling)
  # If a scene has more frames, randomly sample to this limit.
  # Set to -1 to disable sampling.
  # max_frames: 100
  # max_frames: 50
  # max_frames: 10
  max_frames: 3
  # max_frames: 1

  # Only run evaluation (skip inference)
  eval_only: false

  # Only print saved metrics (skip inference and evaluation)
  print_only: false

# ==============================================================================
# Inference Configuration
# ==============================================================================
inference:
  # Number of parallel workers for TSDF fusion
  num_fusion_workers: 4

  # Enable debug mode with verbose output
  # debug: false
  debug: true

# ==============================================================================
# Preset Configurations
# ==============================================================================
# These can be activated via command line: --preset full_eval

presets:
  # Full evaluation on all 6 datasets
  full_eval:
    datasets: [eth3d, 7scenes, scannetpp, hiroom, dtu, dtu64]
    modes: [pose, recon_unposed, recon_posed]

  # Pose-only evaluation
  pose_only:
    datasets: [eth3d, 7scenes, scannetpp, hiroom, dtu64]
    modes: [pose]

  # Reconstruction-only evaluation (5 datasets, excluding dtu64)
  recon_only:
    datasets: [eth3d, 7scenes, scannetpp, hiroom, dtu]
    modes: [recon_unposed, recon_posed]

  # Quick test (single scene per dataset)
  quick_test:
    datasets: [eth3d]
    modes: [pose, recon_unposed]
    scenes: [courtyard]
